{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMALIZE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/586.9 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 524.3/586.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 586.9/586.9 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL ... via @USER\n"
     ]
    }
   ],
   "source": [
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "\n",
    "def normalizeTweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"cannot \", \"can not \")\n",
    "        .replace(\"n't \", \" n't \")\n",
    "        .replace(\"n 't \", \" n't \")\n",
    "        .replace(\"ca n't\", \"can't\")\n",
    "        .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"'m \", \" 'm \")\n",
    "        .replace(\"'re \", \" 're \")\n",
    "        .replace(\"'s \", \" 's \")\n",
    "        .replace(\"'ll \", \" 'll \")\n",
    "        .replace(\"'d \", \" 'd \")\n",
    "        .replace(\"'ve \", \" 've \")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "        .replace(\" p . m \", \" p.m \")\n",
    "        .replace(\" a . m .\", \" a.m.\")\n",
    "        .replace(\" a . m \", \" a.m \")\n",
    "    )\n",
    "\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\n",
    "        normalizeTweet(\n",
    "            \"SC has first two presumptive cases of coronavirus, DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data_i(file_path, indicators):\n",
    "    df = pd.read_csv(file_path)\n",
    "    r_label_cols = ['R1-1-1', 'R2-1-2', 'R3-2-1', 'R4-2-2', 'R5-3-1', 'R6-3-2',\n",
    "                    'R7-3-3', 'R8-4-1', 'R9-4-2', 'R10-5-1', 'R11-5-2', 'R12-5-3']\n",
    "    i_label_cols = ['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12']\n",
    "\n",
    "    r_labels = (np.array(df[r_label_cols])).transpose()\n",
    "    i_labels = (np.array(df[i_label_cols])).transpose()\n",
    "    texts = np.array([t.strip() for t in df['tokenized tweet']])\n",
    "\n",
    "    text_input, target_input, labels, target_idx = [], [], [], []\n",
    "    for i in range(12):\n",
    "        related_mask = r_labels[i] == 1\n",
    "        related_num = np.sum(r_labels[i])\n",
    "\n",
    "        text_input += list(texts[related_mask])\n",
    "        target_input += [indicators[i]] * related_num\n",
    "        labels += list(i_labels[i][related_mask])\n",
    "        target_idx += [i] * related_num\n",
    "\n",
    "    return text_input, target_input, labels, target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, BertModel, RobertaModel\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class IdeologyNet(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(IdeologyNet, self).__init__()\n",
    "\n",
    "        self.plm_encoder = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        self.cls_l1 = nn.Linear(768, 128)\n",
    "        self.cls_l2 = nn.Linear(128, 3)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.act_fun = nn.GELU()\n",
    "        for param in self.plm_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, ids_text, mask_text):\n",
    "        text_embeds = self.plm_encoder(ids_text, mask_text).last_hidden_state\n",
    "\n",
    "        text_reps = text_embeds[:, 0, :]  # [bs, 768]\n",
    "\n",
    "        out = self.cls_l1(text_reps)\n",
    "        out = self.act_fun(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.cls_l2(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367c8142d3b244088aadb72b6a6656c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   6%|6         | 31.5M/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(42)\n",
    "\n",
    "# # tokenizer\n",
    "# if args.plm == 'bertweet':\n",
    "#     tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "# elif args.plm == 'bert':\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# elif args.plm == 'roberta':\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "# load indicators\n",
    "indicators_file = open('C:\\Learn\\IPD\\\\backend\\\\tools\\MITweet\\data\\\\random_split\\Indicators.txt', encoding='utf-8')\n",
    "indicators = [' '.join(line.strip('\\n').strip().split(' ')[:18]) for line in indicators_file]\n",
    "indicators = [ind.replace(' ', '</s> ') for ind in indicators]\n",
    "\n",
    "\n",
    "# load data\n",
    "train_texts, train_targets, train_labels, train_target_idx = load_data_i('testing\\\\train.csv', indicators)\n",
    "val_texts, val_targets, val_labels, val_target_idx = load_data_i('testing\\\\val.csv', indicators)\n",
    "test_texts, test_targets, test_labels, test_target_idx = load_data_i('testing\\\\test.csv', indicators)\n",
    "\n",
    "train_labels, val_labels, test_labels = torch.LongTensor(train_labels), torch.LongTensor(val_labels), torch.LongTensor(test_labels)\n",
    "train_target_idx, val_target_idx, test_target_idx = torch.LongTensor(train_target_idx), torch.LongTensor(val_target_idx), torch.LongTensor(test_target_idx)\n",
    "train_size = len(train_texts)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# model\n",
    "net = IdeologyNet().cuda()\n",
    "multi_gpu = False\n",
    "\n",
    "# optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\", \"layer_norm.bias\", \"layer_norm.weight\"]\n",
    "param_all = list(net.module.named_parameters() if multi_gpu else net.named_parameters())\n",
    "param_plm_encoder = [(n, p) for n, p in param_all if 'encoder' in n]\n",
    "param_other = [(n, p) for n, p in param_all if not ('encoder' in n)]\n",
    "del param_all\n",
    "optimizer_grouped_params = [\n",
    "        {\"params\": [p for n, p in param_plm_encoder if not any(nd in n for nd in no_decay)],\n",
    "         \"lr\": 2e-5,\n",
    "         \"weight_decay\": 0.01},\n",
    "        {\"params\": [p for n, p in param_plm_encoder if any(nd in n for nd in no_decay)],\n",
    "         \"lr\": 2e-5,\n",
    "         \"weight_decay\": 0.0},\n",
    "        {\"params\": [p for n, p in param_other if not any(nd in n for nd in no_decay)],\n",
    "         \"lr\": 1e-3,\n",
    "         \"weight_decay\": 5e-4},\n",
    "        {\"params\": [p for n, p in param_other if any(nd in n for nd in no_decay)],\n",
    "         \"lr\": 1e-3,\n",
    "         \"weight_decay\": 0.0}\n",
    "]\n",
    "del param_plm_encoder, param_other\n",
    "optimizer = AdamW(optimizer_grouped_params, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(texts, targets, labels, target_idx, task='Val'):\n",
    "    net.eval()\n",
    "    data_size = len(texts)\n",
    "\n",
    "    loss_epoch = 0\n",
    "    pred_epoch = None\n",
    "\n",
    "    all_indices = torch.arange(data_size).split(32)\n",
    "    for batch_indices in all_indices:\n",
    "        batch_text = [texts[i] for i in batch_indices]\n",
    "        batch_target = [targets[i] for i in batch_indices]\n",
    "        encode_dict = tokenizer(batch_text, text_pair=batch_target, padding=True, truncation='only_first',\n",
    "                                max_length=128, return_tensors='pt')\n",
    "\n",
    "        out = net(encode_dict['input_ids'].cuda(), encode_dict['attention_mask'].cuda())\n",
    "\n",
    "        batch_label = labels[batch_indices].cuda()\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "        _, pred = torch.max(out, dim=1)\n",
    "        if pred_epoch is None:\n",
    "            pred_epoch = pred.cpu()\n",
    "        else:\n",
    "            pred_epoch = torch.cat((pred_epoch, pred.cpu()), dim=0)\n",
    "\n",
    "    loss_epoch /= (data_size // 32 + 1)\n",
    "    results = compute_performance(pred_epoch, labels.clone(), target_idx.clone())\n",
    "    print(f\"{task}: loss={loss_epoch:.4f}, avg_acc={results['avg_acc']:.4f}, avg_f1={results['avg_f1']:.4f}, \"\n",
    "             f\"global_acc={results['global_acc']:.4f}, global_f1={results['global_f1']:.4f}, \"\n",
    "             f\"global_p={results['global_p']:.4f}, global_r={results['global_r']:.4f}, \"\n",
    "             f\"acc={results['acc']}, f1={results['f1']}, p={results['p']}, r={results['r']}\")\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def train():\n",
    "    best_epoch = 0\n",
    "    best_val_global_f1 = 0\n",
    "    report_avg_acc, report_avg_f1 = 0, 0\n",
    "    report_global_acc, report_global_f1, report_global_p, report_global_r = 0, 0, 0, 0\n",
    "    report_acc, report_f1, report_p, report_r = None, None, None, None\n",
    "\n",
    "    for epoch in range(15):\n",
    "        print(f'\\nEpoch: {epoch+1}')\n",
    "\n",
    "        print(f\"lr_plm: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "        print(f\"lr_other: {optimizer.state_dict()['param_groups'][2]['lr']}\")\n",
    "\n",
    "        loss_epoch = 0\n",
    "        pred_epoch, truth_epoch, target_idx_epoch = None, None, None\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        net.train()\n",
    "        all_indices = torch.randperm(train_size).split(32)\n",
    "        step = 0\n",
    "        for batch_indices in tqdm.tqdm(all_indices, desc=\"batch\"):\n",
    "            step += 1\n",
    "            batch_text = [train_texts[i] for i in batch_indices]\n",
    "            batch_target = [train_targets[i] for i in batch_indices]\n",
    "            encode_dict = tokenizer(batch_text, text_pair=batch_target, padding=True, truncation='only_first',\n",
    "                                    max_length=128, return_tensors='pt')\n",
    "\n",
    "            out = net(encode_dict['input_ids'].cuda(), encode_dict['attention_mask'].cuda())\n",
    "\n",
    "            batch_label = train_labels[batch_indices].cuda()\n",
    "            loss = criterion(out, batch_label)\n",
    "\n",
    "            # optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            _, pred = torch.max(out, dim=1)\n",
    "            batch_target_idx = train_target_idx[batch_indices]\n",
    "            if pred_epoch is None:\n",
    "                pred_epoch, truth_epoch, target_idx_epoch = pred.cpu(), batch_label.cpu(), batch_target_idx.cpu()\n",
    "            else:\n",
    "                pred_epoch = torch.cat((pred_epoch, pred.cpu()), dim=0)\n",
    "                truth_epoch = torch.cat((truth_epoch, batch_label.cpu()), dim=0)\n",
    "                target_idx_epoch = torch.cat((target_idx_epoch, batch_target_idx.cpu()), dim=0)\n",
    "\n",
    "            if step % (4000 // 32) == 0:\n",
    "                num_steps = 4000 // 32\n",
    "                loss_epoch /= num_steps\n",
    "                results = compute_performance(pred_epoch, truth_epoch, target_idx_epoch)\n",
    "                print(f\"Checkpoint: loss={loss_epoch:.4f}, \"\n",
    "                         f\"avg_acc={results['avg_acc']:.4f}, avg_f1={results['avg_f1']:.4f}, \"\n",
    "                         f\"global_acc={results['global_acc']:.4f}, global_f1={results['global_f1']:.4f}, \"\n",
    "                         f\"global_p={results['global_p']:.4f}, global_r={results['global_r']:.4f}, \"\n",
    "                         f\"acc={results['acc']}, f1={results['f1']}, p={results['p']}, r={results['r']}\")\n",
    "                loss_epoch = 0\n",
    "                pred_epoch, truth_epoch, target_idx_epoch = None, None, None\n",
    "\n",
    "        val_results = evaluate(val_texts, val_targets, val_labels, val_target_idx, task='Val')\n",
    "        test_results = evaluate(test_texts, test_targets, test_labels, test_target_idx, task='Test')\n",
    "        if val_results['global_f1'] > best_val_global_f1:\n",
    "            best_epoch = epoch + 1\n",
    "            best_val_global_f1 = val_results['global_f1']\n",
    "            report_avg_acc, report_avg_f1 = test_results['avg_acc'], test_results['avg_f1']\n",
    "            report_global_acc, report_global_f1,  = test_results['global_acc'], test_results['global_f1']\n",
    "            report_global_p, report_global_r = test_results['global_p'], test_results['global_r']\n",
    "            report_acc, report_f1, report_p, report_r = test_results['acc'], test_results['f1'], test_results['p'], test_results['r']\n",
    "\n",
    "        end = time.time()\n",
    "        print('Training Time: {:.2f}s'.format(end - start))\n",
    "\n",
    "    print(f\"\\nReport: best_epoch={best_epoch}\\n\"\n",
    "             f\"avg_acc={report_avg_acc:.4f}, avg_f1={report_avg_f1:.4f}, \"\n",
    "             f\"global_acc={report_global_acc:.4f}, global_f1={report_global_f1:.4f}, \"\n",
    "             f\"global_p={report_global_p:.4f}, global_r={report_global_r:.4f}, \"\n",
    "             f\"avg_p={np.mean([x for x in report_p if x >= 0]):.4f}, avg_r={np.mean([x for x in report_r if x >= 0]):.4f}, \"\n",
    "             f\"acc={report_acc}, f1={report_f1}, p={report_p}, r={report_r}\")\n",
    "\n",
    "\n",
    "def compute_performance(pred, truth, target_idx):\n",
    "    num_sample = len(pred)\n",
    "    num_correct_total = (pred == truth).sum()\n",
    "    global_acc = num_correct_total / num_sample\n",
    "    global_f1 = f1_score(truth, pred, average='macro')\n",
    "    global_p = precision_score(truth, pred, average='macro', zero_division=0)\n",
    "    global_r = recall_score(truth, pred, average='macro', zero_division=0)\n",
    "\n",
    "    acc, f1, p, r = [], [], [], []\n",
    "    for i in range(12):\n",
    "        mask_i = target_idx == i\n",
    "        if mask_i.sum() == 0:\n",
    "            acc.append(-1)\n",
    "            f1.append(-1)\n",
    "            p.append(-1)\n",
    "            r.append(-1)\n",
    "            continue\n",
    "        pred_i = pred[mask_i]\n",
    "        truth_i = truth[mask_i]\n",
    "        label_set_i = list(set(truth_i.numpy()))  # labels that present in current dimension\n",
    "        assert mask_i.sum() == len(pred_i)\n",
    "        acc.append(((pred_i == truth_i).sum() / mask_i.sum()).item())\n",
    "        f1.append(f1_score(truth_i, pred_i, average='macro', labels=label_set_i))\n",
    "        p.append(precision_score(truth_i, pred_i, average='macro', labels=label_set_i, zero_division=0))\n",
    "        r.append(recall_score(truth_i, pred_i, average='macro', labels=label_set_i, zero_division=0))\n",
    "\n",
    "    avg_acc = np.mean([x for x in acc if x >= 0])\n",
    "    avg_f1 = np.mean([x for x in f1 if x >= 0])\n",
    "\n",
    "    result_dict = {'avg_acc': avg_acc, 'avg_f1': avg_f1, 'global_acc': global_acc, 'global_f1': global_f1,\n",
    "                   'global_p': global_p, 'global_r': global_r,\n",
    "                   'acc': [round(x, 4) for x in acc], 'f1': [round(x, 4) for x in f1], 'p': [round(x, 4) for x in p],\n",
    "                   'r': [round(x, 4) for x in r]}\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "lr_plm: 2e-05\n",
      "lr_other: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  33%|███▎      | 125/375 [01:18<02:46,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.9935, avg_acc=0.4850, avg_f1=0.2748, global_acc=0.5195, global_f1=0.3532, global_p=0.4127, global_r=0.3905, acc=[0.52, 0.2877, 0.3892, 0.651, 0.6018, 0.3684, 0.5455, 0.369, 0.2606, 0.6798, 0.7997, 0.3474], f1=[0.4792, 0.2111, 0.2688, 0.3144, 0.3177, 0.1944, 0.2353, 0.2631, 0.2163, 0.2901, 0.3048, 0.2023], p=[0.55, 0.293, 0.3415, 0.2828, 0.3582, 0.1458, 0.1875, 0.3078, 0.3974, 0.3389, 0.2971, 0.3105], r=[0.5321, 0.3453, 0.3497, 0.3572, 0.3423, 0.2917, 0.3158, 0.308, 0.3518, 0.3229, 0.316, 0.3198]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  67%|██████▋   | 250/375 [02:45<02:02,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.8847, avg_acc=0.5492, avg_f1=0.2686, global_acc=0.6340, global_f1=0.4542, global_p=0.4807, global_r=0.4902, acc=[0.4375, 0.2069, 0.3791, 0.7746, 0.5849, 0.5, 0.6176, 0.3908, 0.4846, 0.6732, 0.8586, 0.6827], f1=[0.2029, 0.1358, 0.2355, 0.3747, 0.2796, 0.2222, 0.2545, 0.2894, 0.2831, 0.288, 0.3462, 0.3108], p=[0.1458, 0.1538, 0.2172, 0.3373, 0.2552, 0.1944, 0.2059, 0.3507, 0.3524, 0.3377, 0.3345, 0.4743], r=[0.3333, 0.2764, 0.3327, 0.441, 0.3143, 0.2593, 0.3333, 0.3365, 0.3448, 0.3403, 0.3591, 0.3391]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 375/375 [04:48<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.8341, avg_acc=0.5403, avg_f1=0.2742, global_acc=0.6673, global_f1=0.4885, global_p=0.5339, global_r=0.5229, acc=[0.2609, 0.2692, 0.3744, 0.7197, 0.613, 0.3077, 0.6585, 0.3371, 0.566, 0.6894, 0.8744, 0.8135], f1=[0.202, 0.2104, 0.2781, 0.3683, 0.303, 0.1569, 0.3111, 0.2582, 0.2777, 0.2733, 0.3387, 0.313], p=[0.3026, 0.5192, 0.3595, 0.3841, 0.2862, 0.1212, 0.3176, 0.2802, 0.3497, 0.2358, 0.333, 0.3058], r=[0.4048, 0.3189, 0.3592, 0.4004, 0.3344, 0.2222, 0.3398, 0.2927, 0.3254, 0.325, 0.3481, 0.3238]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: loss=0.7501, avg_acc=0.5664, avg_f1=0.2352, global_acc=0.7063, global_f1=0.5096, global_p=0.4792, global_r=0.5546, acc=[0.375, 0.1667, 0.4153, 0.7604, 0.6629, 0.4545, 0.5455, 0.2941, 0.6236, 0.7224, 0.8745, 0.902], f1=[0.1818, 0.0952, 0.1956, 0.288, 0.2658, 0.2083, 0.2353, 0.189, 0.2561, 0.2796, 0.311, 0.3162], p=[0.125, 0.0556, 0.1384, 0.2535, 0.221, 0.1515, 0.1818, 0.1596, 0.2079, 0.2408, 0.2915, 0.3007], r=[0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.2373, 0.3333, 0.3333, 0.3333, 0.3333]\n",
      "Test: loss=0.7334, avg_acc=0.5713, avg_f1=0.2369, global_acc=0.7195, global_f1=0.5173, global_p=0.4833, global_r=0.5617, acc=[0.25, 0.2059, 0.3455, 0.7732, 0.7123, 0.4545, 0.625, 0.3333, 0.6333, 0.7429, 0.8962, 0.8835], f1=[0.1333, 0.1138, 0.1712, 0.2907, 0.2773, 0.2083, 0.2564, 0.2208, 0.2585, 0.2842, 0.3151, 0.3127], p=[0.0833, 0.0686, 0.1152, 0.2577, 0.2374, 0.1515, 0.2083, 0.1914, 0.2127, 0.2476, 0.2987, 0.2951], r=[0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.2613, 0.3295, 0.3333, 0.3333, 0.3326]\n",
      "Training Time: 455.39s\n",
      "\n",
      "Epoch: 2\n",
      "lr_plm: 2e-05\n",
      "lr_other: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  33%|███▎      | 125/375 [02:25<05:07,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.8024, avg_acc=0.5751, avg_f1=0.3042, global_acc=0.6892, global_f1=0.5194, global_p=0.5784, global_r=0.5474, acc=[0.4583, 0.3385, 0.3886, 0.7718, 0.6153, 0.2727, 0.6667, 0.3829, 0.587, 0.7163, 0.865, 0.8381], f1=[0.2961, 0.2772, 0.2596, 0.3191, 0.3031, 0.1429, 0.468, 0.309, 0.2992, 0.3003, 0.3224, 0.354], p=[0.3651, 0.3022, 0.315, 0.3905, 0.4414, 0.125, 0.5882, 0.347, 0.3628, 0.4629, 0.3138, 0.3463], r=[0.3667, 0.4444, 0.328, 0.3234, 0.3334, 0.1667, 0.5208, 0.3292, 0.337, 0.3415, 0.3362, 0.3633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  67%|██████▋   | 250/375 [05:02<02:54,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.7919, avg_acc=0.5788, avg_f1=0.2796, global_acc=0.6873, global_f1=0.5088, global_p=0.5891, global_r=0.5391, acc=[0.381, 0.3333, 0.4011, 0.673, 0.6307, 0.5294, 0.6216, 0.3651, 0.6098, 0.6758, 0.8742, 0.8508], f1=[0.264, 0.2538, 0.2917, 0.2733, 0.2746, 0.2308, 0.2556, 0.297, 0.3171, 0.2692, 0.3114, 0.3169], p=[0.3542, 0.2556, 0.4455, 0.246, 0.2551, 0.1765, 0.2323, 0.3765, 0.4475, 0.2281, 0.2961, 0.3053], r=[0.4, 0.3075, 0.3665, 0.3075, 0.321, 0.3333, 0.284, 0.3124, 0.3581, 0.3285, 0.3283, 0.3294]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 375/375 [07:37<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.7931, avg_acc=0.5535, avg_f1=0.2859, global_acc=0.6754, global_f1=0.5204, global_p=0.5718, global_r=0.5449, acc=[0.4286, 0.2821, 0.3687, 0.7544, 0.633, 0.3889, 0.4571, 0.3522, 0.5634, 0.7013, 0.8793, 0.8331], f1=[0.2694, 0.2381, 0.2815, 0.3447, 0.2912, 0.2406, 0.2457, 0.2993, 0.3067, 0.2978, 0.3121, 0.3034], p=[0.3046, 0.2864, 0.3299, 0.4662, 0.5994, 0.2444, 0.2674, 0.3114, 0.3843, 0.6356, 0.2965, 0.2907], r=[0.3413, 0.3191, 0.3388, 0.3609, 0.3319, 0.2976, 0.3198, 0.3114, 0.3488, 0.343, 0.3295, 0.3172]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: loss=0.7218, avg_acc=0.5987, avg_f1=0.2990, global_acc=0.7157, global_f1=0.5529, global_p=0.6137, global_r=0.5817, acc=[0.5, 0.5833, 0.4322, 0.7604, 0.6562, 0.1818, 0.6364, 0.3077, 0.6236, 0.7265, 0.8745, 0.902], f1=[0.3651, 0.3106, 0.3631, 0.3632, 0.2642, 0.1212, 0.3778, 0.2351, 0.2561, 0.3041, 0.311, 0.3162], p=[0.3333, 0.3111, 0.4047, 0.3692, 0.2202, 0.0833, 0.3889, 0.3738, 0.2079, 0.5751, 0.2915, 0.3007], r=[0.4048, 0.375, 0.3931, 0.3843, 0.33, 0.2222, 0.4167, 0.3247, 0.3333, 0.3457, 0.3333, 0.3333]\n",
      "Test: loss=0.7205, avg_acc=0.5934, avg_f1=0.2837, global_acc=0.7317, global_f1=0.5706, global_p=0.6719, global_r=0.5973, acc=[0.375, 0.4118, 0.5636, 0.7526, 0.7123, 0.1818, 0.625, 0.3376, 0.6407, 0.7393, 0.8962, 0.8854], f1=[0.1905, 0.1944, 0.4498, 0.3615, 0.31, 0.15, 0.3259, 0.2507, 0.2603, 0.2834, 0.3151, 0.3131], p=[0.1667, 0.1458, 0.4461, 0.3275, 0.3819, 0.1528, 0.3333, 0.3, 0.2136, 0.2473, 0.2987, 0.2951], r=[0.2222, 0.2917, 0.4887, 0.4033, 0.3486, 0.2333, 0.3587, 0.3575, 0.3333, 0.3317, 0.3333, 0.3333]\n",
      "Training Time: 632.92s\n",
      "\n",
      "Epoch: 3\n",
      "lr_plm: 2e-05\n",
      "lr_other: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  33%|███▎      | 125/375 [02:38<04:27,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.7763, avg_acc=0.6065, avg_f1=0.3016, global_acc=0.6933, global_f1=0.5177, global_p=0.5704, global_r=0.5462, acc=[0.5769, 0.2535, 0.449, 0.8025, 0.6596, 0.5385, 0.6154, 0.3943, 0.5845, 0.7019, 0.8692, 0.8325], f1=[0.4196, 0.1943, 0.3288, 0.2968, 0.2703, 0.3556, 0.254, 0.3026, 0.2671, 0.316, 0.3102, 0.3041], p=[0.5167, 0.1822, 0.4035, 0.2763, 0.2407, 0.5, 0.2222, 0.3309, 0.2546, 0.4257, 0.2939, 0.2925], r=[0.4389, 0.2867, 0.3661, 0.3206, 0.3194, 0.4167, 0.2963, 0.3265, 0.322, 0.3523, 0.3285, 0.3167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  67%|██████▋   | 250/375 [05:04<02:32,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.7809, avg_acc=0.5601, avg_f1=0.2830, global_acc=0.6823, global_f1=0.5230, global_p=0.5871, global_r=0.5468, acc=[0.3793, 0.3824, 0.3671, 0.7219, 0.627, 0.375, 0.5306, 0.366, 0.5728, 0.6851, 0.8707, 0.843], f1=[0.2569, 0.2831, 0.3013, 0.2795, 0.2818, 0.1818, 0.2561, 0.3129, 0.278, 0.3048, 0.3287, 0.3307], p=[0.3283, 0.2637, 0.3347, 0.2455, 0.2693, 0.1429, 0.2561, 0.3521, 0.3115, 0.475, 0.6278, 0.3203], r=[0.3074, 0.3444, 0.3442, 0.3244, 0.3289, 0.25, 0.3049, 0.3322, 0.3199, 0.3479, 0.3377, 0.3431]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 375/375 [07:34<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: loss=0.7616, avg_acc=0.5682, avg_f1=0.2899, global_acc=0.6871, global_f1=0.5226, global_p=0.5979, global_r=0.5492, acc=[0.44, 0.3, 0.322, 0.7544, 0.6192, 0.4118, 0.5758, 0.3725, 0.5768, 0.7121, 0.8847, 0.8498], f1=[0.3115, 0.2459, 0.2701, 0.347, 0.2756, 0.1944, 0.2962, 0.2858, 0.3348, 0.2888, 0.3129, 0.3159], p=[0.3492, 0.2033, 0.3047, 0.4191, 0.2639, 0.1556, 0.281, 0.3349, 0.4617, 0.2958, 0.2979, 0.3046], r=[0.3968, 0.3212, 0.3054, 0.3678, 0.327, 0.2593, 0.3274, 0.3034, 0.3688, 0.3349, 0.3296, 0.3283]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: loss=0.7053, avg_acc=0.5864, avg_f1=0.2754, global_acc=0.7176, global_f1=0.5831, global_p=0.6499, global_r=0.5881, acc=[0.4375, 0.2083, 0.4492, 0.7604, 0.6629, 0.4545, 0.5455, 0.4027, 0.6162, 0.7224, 0.8745, 0.902], f1=[0.2738, 0.1642, 0.3049, 0.288, 0.2658, 0.2083, 0.2353, 0.2932, 0.3651, 0.2796, 0.311, 0.3162], p=[0.4667, 0.2117, 0.2958, 0.2535, 0.221, 0.1515, 0.1818, 0.2712, 0.3743, 0.2408, 0.2915, 0.3007], r=[0.381, 0.2701, 0.3759, 0.3333, 0.3333, 0.3333, 0.3333, 0.3486, 0.375, 0.3333, 0.3333, 0.3333]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 178\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m         pred_epoch, truth_epoch, target_idx_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    177\u001b[0m val_results \u001b[38;5;241m=\u001b[39m evaluate(val_texts, val_targets, val_labels, val_target_idx, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_target_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_f1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m best_val_global_f1:\n\u001b[0;32m    180\u001b[0m     best_epoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 92\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(texts, targets, labels, target_idx, task)\u001b[0m\n\u001b[0;32m     88\u001b[0m batch_target \u001b[38;5;241m=\u001b[39m [targets[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n\u001b[0;32m     89\u001b[0m encode_dict \u001b[38;5;241m=\u001b[39m tokenizer(batch_text, text_pair\u001b[38;5;241m=\u001b[39mbatch_target, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly_first\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m batch_label \u001b[38;5;241m=\u001b[39m labels[batch_indices]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, batch_label)\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mIdeologyNet.forward\u001b[1;34m(self, ids_text, mask_text)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids_text, mask_text):\n\u001b[1;32m---> 22\u001b[0m     text_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplm_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_text\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     24\u001b[0m     text_reps \u001b[38;5;241m=\u001b[39m text_embeds[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# [bs, 768]\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_l1(text_reps)\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    831\u001b[0m )\n\u001b[1;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    512\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m         output_attentions,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:410\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    400\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    409\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:337\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    329\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    335\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 337\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:187\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    179\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 187\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ranah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
